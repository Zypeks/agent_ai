{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uO0YNpNkV8r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_shares_INTC = pd.read_csv(\"INTC.csv\")\n",
        "df_shares_INTC.name = \"intel\"\n",
        "df_shares_IBM = pd.read_csv(\"IBM.csv\")\n",
        "df_shares_IBM.name = \"ibm\"\n",
        "df_shares_NVDA = pd.read_csv(\"NVDA.csv\")\n",
        "df_shares_NVDA.name = \"nvidia\"\n",
        "df_shares_AMD = pd.read_csv(\"AMD.csv\")\n",
        "df_shares_META = pd.read_csv(\"META.csv\")\n",
        "df_shares_GOOGLE = pd.read_csv(\"alphabet.csv\")\n",
        "df_shares_CISCO = pd.read_csv(\"CSCO.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gym-anytrading\n",
        "%pip install stable-baselines3\n",
        "\n",
        "url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n",
        "url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3\n",
        "import talib\n",
        "\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "7jtmicHfkbx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gym_anytrading.envs import StocksEnv, Actions, Positions\n",
        "\n",
        "def reshape_obs(state, action, actual_ws):\n",
        "    reshaped_state = state[:, 0]\n",
        "    ws = len(reshaped_state)\n",
        "\n",
        "    rsi_timeperiod = 9\n",
        "    rsi_9 = talib.RSI(np.float64(reshaped_state[ws - rsi_timeperiod - 1:ws]), timeperiod=rsi_timeperiod)[-1]\n",
        "    rsi_9 = rsi_9 / 100\n",
        "\n",
        "    sma_200d = np.float32(talib.SMA(np.array(reshaped_state, dtype=np.float64), timeperiod=200))[-1]\n",
        "    sma_200d = (reshaped_state[-1] / sma_200d) - 1\n",
        "\n",
        "    reshaped_state = reshaped_state[ws - actual_ws:ws]\n",
        "\n",
        "    reshaped_state = (reshaped_state - np.mean(reshaped_state)) / np.std(reshaped_state)\n",
        "    reshaped_state = np.append(reshaped_state, action)\n",
        "    reshaped_state = np.append(reshaped_state, rsi_9)\n",
        "    reshaped_state = np.append(reshaped_state, sma_200d)\n",
        "    return reshaped_state\n",
        "\n",
        "class MyStocksEnv(StocksEnv):\n",
        "    def __init__(self, actual_ws, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.actual_ws = actual_ws\n",
        "        self.shape = (self.actual_ws + 3, )\n",
        "        INF = 1e10\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.last_action = 0\n",
        "\n",
        "        self.number_of_trades = 0\n",
        "        self.days_holding_position = 1\n",
        "        self.history_days_holding_position = []\n",
        "\n",
        "        self._total_reward = 0.\n",
        "        self.trade_fee_bid_percent = 0\n",
        "        self.trade_fee_ask_percent = 0\n",
        "\n",
        "\n",
        "    def is_trade(self, action):\n",
        "        if (action == Actions.Sell.value and self._position == Positions.Long):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def _calculate_reward(self, action):\n",
        "        step_reward = 0\n",
        "\n",
        "        if self.is_trade(action) or (self._truncated and self._position == Positions.Long):\n",
        "            self.number_of_trades += 1\n",
        "            current_price = self.prices[self._current_tick]\n",
        "            last_trade_price = self.prices[self._last_trade_tick]\n",
        "            step_reward = (current_price / last_trade_price) - 1\n",
        "\n",
        "        return step_reward\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed, options=options)\n",
        "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
        "\n",
        "        self.number_of_trades = 0\n",
        "        self.days_holding_position = 1\n",
        "        self.history_days_holding_position = []\n",
        "\n",
        "        self._truncated = False\n",
        "        self._current_tick = self._start_tick\n",
        "        self._last_trade_tick = self._current_tick - 1\n",
        "        self._position = Positions.Short\n",
        "        self._position_history = (self.window_size * [None]) + [self._position]\n",
        "        self._total_reward = 0.\n",
        "        self._total_profit = 1.  # unit\n",
        "        self._first_rendering = True\n",
        "        self.history = {}\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        observation = reshape_obs(observation, 0, self.actual_ws)\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == 'human':\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._truncated = False\n",
        "        self._current_tick += 1\n",
        "\n",
        "        if self._current_tick == self._end_tick:\n",
        "            self.history_days_holding_position.append(self.days_holding_position)\n",
        "            self._truncated = True\n",
        "\n",
        "        step_reward = self._calculate_reward(action)\n",
        "        self._total_reward += step_reward\n",
        "\n",
        "        self._update_profit(action)\n",
        "\n",
        "        trade = False\n",
        "        if (\n",
        "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
        "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
        "        ):\n",
        "            trade = True\n",
        "\n",
        "        if trade:\n",
        "            self._position = self._position.opposite()\n",
        "            self._last_trade_tick = self._current_tick\n",
        "\n",
        "        self._position_history.append(self._position)\n",
        "        observation = self._get_observation()\n",
        "        observation = reshape_obs(observation, action, self.actual_ws)\n",
        "        info = self._get_info()\n",
        "        self._update_history(info)\n",
        "\n",
        "        if self.last_action == action:\n",
        "            self.days_holding_position += 1\n",
        "        else:\n",
        "            self.history_days_holding_position.append(self.days_holding_position)\n",
        "            self.days_holding_position = 1\n",
        "            self.last_action = action\n",
        "\n",
        "        if self.render_mode == 'human':\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, step_reward, self._truncated, self._truncated, info\n",
        "\n",
        "    def get_stats(self):\n",
        "        return self.number_of_trades, np.mean(self.history_days_holding_position), self.history_days_holding_position\n",
        "\n",
        "    def get_trend(self):\n",
        "        if self.prices[self.window_size] - self.prices[-1] > 0:\n",
        "            return \"decreasing\"\n",
        "        return \"rising\"\n",
        "\n",
        "    def render_all(self, max_days, title=None):\n",
        "        figsize_x = math.ceil((max_days / 100) * 3)\n",
        "        plt.figure(figsize=(figsize_x, 6))\n",
        "        window_ticks = np.arange(len(self._position_history))\n",
        "        plt.plot(self.prices)\n",
        "\n",
        "        short_ticks = []\n",
        "        long_ticks = []\n",
        "        last_pos = None\n",
        "        for i, tick in enumerate(window_ticks):\n",
        "            current_pos = self._position_history[i]\n",
        "\n",
        "            if current_pos == last_pos:\n",
        "                continue\n",
        "\n",
        "            if  current_pos == Positions.Short:\n",
        "                short_ticks.append(tick)\n",
        "            elif current_pos == Positions.Long:\n",
        "                long_ticks.append(tick)\n",
        "\n",
        "            last_pos = current_pos\n",
        "\n",
        "        plt.plot(short_ticks, self.prices[short_ticks], 'ro', label=\"Sprzedaj\")\n",
        "        plt.plot(long_ticks, self.prices[long_ticks], 'go', label=\"Kup\")\n",
        "\n",
        "        plt.legend()\n",
        "\n",
        "        if title:\n",
        "            plt.title(f\"Stocks from day {self.frame_bound[0]}\")\n",
        "\n",
        "        plt.suptitle(\n",
        "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
        "            \"Total Profit: %.6f\" % self._total_profit\n",
        "        )\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "kPmo9_xRkdLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def create_model(learning_rate, input_dims, output_dims, l1_dims, l2_dims):\n",
        "    model = Sequential(\n",
        "        [\n",
        "            Dense(l1_dims, input_shape=(input_dims,)),\n",
        "            Activation('relu'),\n",
        "            Dense(l2_dims),\n",
        "            Activation('relu'),\n",
        "            Dense(output_dims)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=opt, loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "class SumTree:\n",
        "    def __init__(self, max_size):\n",
        "        self.data_idx = 0\n",
        "        self.max_size = max_size\n",
        "        self.tree = np.zeros(2 * max_size - 1)\n",
        "        self.data = np.zeros(max_size, dtype=np.int32)\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        self.data[self.data_idx] = data\n",
        "\n",
        "        tree_index = self.max_size - 1 + self.data_idx\n",
        "        self.update(tree_index, priority)\n",
        "\n",
        "        self.data_idx = (self.data_idx + 1) % self.max_size\n",
        "\n",
        "    def update(self, tree_index, priority):\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "\n",
        "        while tree_index != 0:\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change\n",
        "\n",
        "    def sample(self, value):\n",
        "        parent_index = 0\n",
        "        while True:\n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "\n",
        "            if left_child_index >= len(self.tree):\n",
        "                leaf_index = parent_index\n",
        "                break\n",
        "\n",
        "            if value <= self.tree[left_child_index]:\n",
        "                parent_index = left_child_index\n",
        "            else:\n",
        "                value -= self.tree[left_child_index]\n",
        "                parent_index = right_child_index\n",
        "\n",
        "        data_index = leaf_index - self.max_size + 1\n",
        "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
        "\n",
        "    def get_total_priority(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "class PrioritizedExpirenceReplay:\n",
        "    def __init__(self, mem_size, state_space_n, action_space_n, alpha=0.6, beta=0.4, beta_incr=0.001):\n",
        "        self.mem_size = mem_size\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.states = np.zeros((self.mem_size, state_space_n))\n",
        "        self.next_states = np.zeros((self.mem_size, state_space_n))\n",
        "        self.actions = np.zeros((self.mem_size, action_space_n), dtype=np.int8)\n",
        "        self.rewards = np.zeros(self.mem_size)\n",
        "        self.dones = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "        self.sum_tree = SumTree(self.mem_size)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.beta_incr = beta_incr\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        mem_idx = self.mem_cntr % self.mem_size\n",
        "\n",
        "        self.states[mem_idx] = state\n",
        "        self.next_states[mem_idx] = next_state\n",
        "        actions = np.zeros(self.actions.shape[1])\n",
        "        actions[action] = 1.0\n",
        "        self.actions[mem_idx] = actions\n",
        "        self.rewards[mem_idx] = reward\n",
        "        self.dones[mem_idx] = 1 - done\n",
        "\n",
        "        priority = np.max(self.sum_tree.tree[-self.sum_tree.max_size:])\n",
        "        if priority == 0:\n",
        "            priority = 1\n",
        "        self.sum_tree.add(priority, mem_idx)\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def get_sample_indicies(self, batch_size):\n",
        "        tree_indicies = []\n",
        "        mem_indices = []\n",
        "        priorities = []\n",
        "        segment = self.sum_tree.get_total_priority() / batch_size\n",
        "        for k in range(batch_size):\n",
        "            section_min, section_max  = segment * k, segment * (k + 1)\n",
        "            seed = random.uniform(section_min, section_max)\n",
        "            tree_indice, priority, mem_indice = self.sum_tree.sample(seed)\n",
        "            tree_indicies.append(tree_indice)\n",
        "            mem_indices.append(mem_indice)\n",
        "            priorities.append(priority)\n",
        "        return np.array(tree_indicies), np.array(mem_indices), np.array(priorities)\n",
        "\n",
        "    def is_memory_prepared(self, batch_size):\n",
        "        return self.mem_cntr >= batch_size\n",
        "\n",
        "    def sample_memory(self, batch_size):\n",
        "        current_mem_size = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "        tree_indicies, mem_indices, mem_priorities = self.get_sample_indicies(batch_size)\n",
        "        priorities_normalized = mem_priorities / self.sum_tree.get_total_priority()\n",
        "\n",
        "        min_priority = np.min(self.sum_tree.tree[-self.sum_tree.max_size:][self.sum_tree.tree[-self.sum_tree.max_size:] != 0])\n",
        "        if min_priority == 0:\n",
        "            min_priority = 1\n",
        "        min_prioritiy_normalized = min_priority / self.sum_tree.get_total_priority()\n",
        "\n",
        "        weights = np.power(current_mem_size * priorities_normalized, -self.beta)\n",
        "        max_weight = np.power(current_mem_size * min_prioritiy_normalized, -self.beta)\n",
        "        weights = weights / max_weight\n",
        "\n",
        "        states = self.states[mem_indices]\n",
        "        actions = self.actions[mem_indices]\n",
        "        rewards = self.rewards[mem_indices]\n",
        "        next_states = self.next_states[mem_indices]\n",
        "        dones = self.dones[mem_indices]\n",
        "\n",
        "        # self.beta = min(self.beta + self.beta_incr, 1.0)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, tree_indicies, weights\n",
        "\n",
        "    def update_priorities(self, mem_indices, priorities, offset=0.001):\n",
        "        for indice, priority in zip(mem_indices, priorities):\n",
        "            priority = priority + offset\n",
        "            priority = np.power(priority, self.alpha)\n",
        "            self.sum_tree.update(indice, priority)\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "            self,\n",
        "            state_space_n,\n",
        "            action_space_n,\n",
        "            learning_rate=0.00001,\n",
        "            l1_dims=256,\n",
        "            l2_dims=256,\n",
        "            gamma=0.99,\n",
        "            epsilon=1.0,\n",
        "            epsilon_decay_rate=0.999,\n",
        "            batch_size=64,\n",
        "            epsilon_min=0.03,\n",
        "            mem_size=10000,\n",
        "            model_sync_freq=250,\n",
        "            alpha=1,\n",
        "            beta=0,\n",
        "            beta_incr=0,\n",
        "        ):\n",
        "        self.action_space_values = [i for i in range(action_space_n)]\n",
        "        self.action_space = np.array(self.action_space_values, dtype=np.int8)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.replay_buffer = PrioritizedExpirenceReplay(mem_size, state_space_n, action_space_n, alpha, beta, beta_incr)\n",
        "\n",
        "        self.q_model = create_model(learning_rate, state_space_n, action_space_n, l1_dims, l2_dims)\n",
        "        self.q_target_model = create_model(learning_rate, state_space_n, action_space_n, l1_dims, l2_dims)\n",
        "        self.model_sync_freq = model_sync_freq\n",
        "\n",
        "        self.q_model_loss = []\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.replay_buffer.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        action = np.random.choice(self.action_space_values)\n",
        "        if np.random.random() >= self.epsilon:\n",
        "            state = state[np.newaxis, :]\n",
        "            actions = self.q_model.predict(state)\n",
        "            action = np.argmax(actions)\n",
        "        return action\n",
        "\n",
        "    def choose_best_action(self, state):\n",
        "        state = state[np.newaxis, :]\n",
        "        actions = self.q_model.predict(state)\n",
        "        return np.argmax(actions)\n",
        "\n",
        "    def learn(self):\n",
        "        if not self.replay_buffer.is_memory_prepared(self.batch_size):\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, new_states, dones, tree_indicies, weights  = self.replay_buffer.sample_memory(self.batch_size)\n",
        "\n",
        "        action_indices = np.dot(actions, self.action_space)\n",
        "\n",
        "        q_pred = self.q_model.predict(states)\n",
        "        old_q_pred = np.array(q_pred)\n",
        "        q_eval = self.q_model.predict(new_states)\n",
        "        q_next = self.q_target_model.predict(new_states)\n",
        "\n",
        "        max_actions = np.argmax(q_eval, axis=1)\n",
        "\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        q_pred[batch_index, action_indices] = rewards + self.gamma * q_next[batch_index, max_actions.astype(int)] * dones\n",
        "\n",
        "        loss = self.q_model.fit(states, q_pred, sample_weight=weights, verbose=0)\n",
        "        self.save_model_loss(loss)\n",
        "\n",
        "        td_errors = abs(q_pred[batch_index, action_indices] - old_q_pred[batch_index, action_indices])\n",
        "\n",
        "        self.replay_buffer.update_priorities(tree_indicies, td_errors)\n",
        "\n",
        "        if self.replay_buffer.mem_cntr % self.model_sync_freq == 0:\n",
        "            self.update_network_parameters()\n",
        "\n",
        "    def save_model_loss(self, loss):\n",
        "        loss = loss.history[\"loss\"]\n",
        "        self.q_model_loss.append(loss)\n",
        "\n",
        "    def update_network_parameters(self):\n",
        "        self.q_target_model.set_weights(self.q_model.get_weights())\n",
        "\n",
        "    def load_model(self, file_name):\n",
        "        self.q_model = load_model(file_name)\n",
        "        self.update_network_parameters()\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay_rate, self.epsilon_min)\n",
        "\n",
        "    def save_model(self, timestep, file_name=\"DDQN_PER_model\"):\n",
        "        self.q_model.save(f\"{file_name}_{timestep}.h5\")"
      ],
      "metadata": {
        "id": "la_TjN0rkkND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_plot(xlabel, ylabel, title, values):\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.plot(range(len(values)), values, \"r\")\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "class AnytradingStatsCollector(object):\n",
        "    def __init__(self):\n",
        "        self.profits = []\n",
        "        self.scores = []\n",
        "        self.epsilons = []\n",
        "\n",
        "        self.stats_cntr = 0\n",
        "\n",
        "    def store_episode_results(self, profit, score, epsilon):\n",
        "        self.profits.append(profit)\n",
        "        self.scores.append(score)\n",
        "        self.epsilons.append(epsilon)\n",
        "\n",
        "        self.stats_cntr += 1\n",
        "\n",
        "    def print_stats(self, backward_episodes=100):\n",
        "        stats_start_idx = max(0, self.stats_cntr-backward_episodes)\n",
        "\n",
        "        avg_profit = np.mean(self.profits[stats_start_idx:self.stats_cntr])\n",
        "        avg_score = np.mean(self.scores[stats_start_idx:self.stats_cntr])\n",
        "\n",
        "        median_profit = np.median(self.profits[stats_start_idx:(self.stats_cntr)])\n",
        "        median_score = np.median(self.scores[stats_start_idx:(self.stats_cntr)])\n",
        "\n",
        "        print(f\"EP: {self.stats_cntr - 1}, epsilon: {self.epsilons[-1]}, score: {self.scores[-1]}, profit: {self.profits[-1]}, avg_score: {avg_score}, avg_profit: {avg_profit},  median_score: {median_score}, median_profit: {median_profit}\")\n",
        "\n",
        "    def print_stats_plots(self):\n",
        "        create_plot(\"Episode\", \"Score\", \"Score per epiosde\", self.scores)\n",
        "        create_plot(\"Episode\", \"Profit\", \"Profit per epiosde\", self.profits)\n",
        "\n",
        "    def get_avg_profit_form_last_10_eps(self):\n",
        "        if self.stats_cntr <= 10:\n",
        "            return 0\n",
        "        return np.mean(self.profits[self.stats_cntr - 10:self.stats_cntr])"
      ],
      "metadata": {
        "id": "TPG8ybu1ktEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(actual_ws, dataframe, ws, start, period):\n",
        "    env = MyStocksEnv(\n",
        "        actual_ws,\n",
        "        df=dataframe,\n",
        "        window_size=ws,\n",
        "        frame_bound=(start, start+period+1)\n",
        "    )\n",
        "    return env"
      ],
      "metadata": {
        "id": "nF0M1HO6kujf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 200\n",
        "start = 200\n",
        "period = len(df_shares_INTC) - 200\n",
        "actual_ws = 100\n",
        "\n",
        "agent = Agent(state_space_n=actual_ws + 3, action_space_n=2)\n",
        "\n",
        "env = make_env(actual_ws, df_shares_INTC, window_size, start, period)\n",
        "anytrading_stats = AnytradingStatsCollector()\n",
        "\n",
        "episodes = 35\n",
        "for ep in range(episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    state = env.reset()[0]\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "\n",
        "        next_state, reward, truncated, done, info = env.step(action)\n",
        "        score += reward\n",
        "\n",
        "        agent.store_transition(state, action, reward, next_state, int(done))\n",
        "        state = next_state\n",
        "\n",
        "        agent.learn()\n",
        "\n",
        "        agent.update_epsilon()\n",
        "\n",
        "    anytrading_stats.store_episode_results(info['total_profit'], score, agent.epsilon)\n",
        "    anytrading_stats.print_stats()\n",
        "\n",
        "agent.save_model(ep)\n",
        "env.render_all(period, f\"Starting day: {start}\")\n",
        "\n",
        "anytrading_stats.print_stats_plots()\n",
        "create_plot(\"Loss\", \"Step\", \"Loss per step\", agent.q_model_loss)"
      ],
      "metadata": {
        "id": "SW7_-gQskws6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shares_storage = {\n",
        "    \"amd\": df_shares_AMD,\n",
        "    \"meta\": df_shares_META,\n",
        "    \"google\": df_shares_GOOGLE,\n",
        "    \"cisco\": df_shares_CISCO,\n",
        "}\n",
        "\n",
        "stats = {\n",
        "    \"profits\": [],\n",
        "    \"num_of_trades\": [],\n",
        "    \"avg_days_holding_positions\": [],\n",
        "    \"trends\": {\n",
        "        \"decreasing\": {\n",
        "            \"counter\": 0,\n",
        "            \"num_of_profits\": 0,\n",
        "            \"profit_values\": [],\n",
        "        },\n",
        "        \"rising\": {\n",
        "            \"counter\": 0,\n",
        "            \"num_of_profits\": 0,\n",
        "            \"profit_values\": [],\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "for key, t_shares in shares_storage.items():\n",
        "  print(f\"--------------------------{key}--------------------------\")\n",
        "  for i in range(200, len(t_shares) - 365, 100):\n",
        "      env = make_env(actual_ws, t_shares, window_size, i, 365)\n",
        "      obs = env.reset()[0]\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "          action = agent.choose_best_action(obs)\n",
        "          obs, reward, truncated, done, info = env.step(action)\n",
        "    #   env.render_all(365, True)\n",
        "\n",
        "      trend = env.get_trend()\n",
        "      num_of_trades, avg_days_holding_positions, _ = env.get_stats()\n",
        "      profit = info[\"total_profit\"]\n",
        "\n",
        "      stats['profits'].append(profit)\n",
        "      stats[\"num_of_trades\"].append(num_of_trades)\n",
        "      stats[\"avg_days_holding_positions\"].append(avg_days_holding_positions)\n",
        "      stats[\"trends\"][trend][\"counter\"] += 1\n",
        "      if profit >= 1.0:\n",
        "        stats[\"trends\"][trend][\"num_of_profits\"] += 1\n",
        "      stats[\"trends\"][trend][\"profit_values\"].append(profit)\n",
        "\n",
        "print(f\"--------------------------STATS--------------------------\")\n",
        "print(f\"Median profit: {np.median(stats['profits'])}\")\n",
        "print(f\"Lowest profit achived: {(np.min(stats['profits']) - 1) * 100}\")\n",
        "print(f\"Highest profit achived: {(np.max(stats['profits']) - 1) * 100}\")\n",
        "print(f\"Average num_of_trades: {np.mean(stats['num_of_trades'])}\")\n",
        "print(f\"Average avg_days_holding_positions: {np.mean(stats['avg_days_holding_positions'])}\")\n",
        "print(f\"Profits achvied on decreasing trend: {stats['trends']['decreasing']['num_of_profits']} / {stats['trends']['decreasing']['counter']}, {stats['trends']['decreasing']['num_of_profits'] / stats['trends']['decreasing']['counter'] * 100}%\")\n",
        "print(f\"Median profit achvied on decreasing trend: {np.median(stats['trends']['decreasing']['profit_values'])}, {(np.median(stats['trends']['decreasing']['profit_values']) - 1) * 100}%\")\n",
        "print(f\"Profits achvied on rising trend: {stats['trends']['rising']['num_of_profits']} / {stats['trends']['rising']['counter']}, {stats['trends']['rising']['num_of_profits'] / stats['trends']['rising']['counter'] * 100}%\")\n",
        "print(f\"Median profit achvied on rising trend: {np.median(stats['trends']['rising']['profit_values'])}, {(np.median(stats['trends']['rising']['profit_values']) - 1) * 100}%\")\n"
      ],
      "metadata": {
        "id": "S0WG7T04X8cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dodanie do stanu SMA 100 dni oraz SMA 50 dni**"
      ],
      "metadata": {
        "id": "qojSG_A-4jVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gym_anytrading.envs import StocksEnv, Actions, Positions\n",
        "\n",
        "def reshape_obs(state, action, actual_ws):\n",
        "    reshaped_state = state[:, 0]\n",
        "    ws = len(reshaped_state)\n",
        "\n",
        "    rsi_timeperiod = 9\n",
        "    rsi_9 = talib.RSI(np.float64(reshaped_state[ws - rsi_timeperiod - 1:ws]), timeperiod=rsi_timeperiod)[-1]\n",
        "    rsi_9 = rsi_9 / 100\n",
        "\n",
        "    sma_200d = np.float32(talib.SMA(np.array(reshaped_state, dtype=np.float64), timeperiod=200))[-1]\n",
        "    sma_200d = (reshaped_state[-1] / sma_200d) - 1\n",
        "\n",
        "    sma_100d = np.float32(talib.SMA(np.array(reshaped_state, dtype=np.float64), timeperiod=100))[-1]\n",
        "    sma_100d = (reshaped_state[-1] / sma_100d) - 1\n",
        "\n",
        "    sma_50d = np.float32(talib.SMA(np.array(reshaped_state, dtype=np.float64), timeperiod=50))[-1]\n",
        "    sma_50d = (reshaped_state[-1] / sma_50d) - 1\n",
        "\n",
        "    reshaped_state = reshaped_state[ws - actual_ws:ws]\n",
        "\n",
        "    reshaped_state = (reshaped_state - np.mean(reshaped_state)) / np.std(reshaped_state)\n",
        "    reshaped_state = np.append(reshaped_state, action)\n",
        "    reshaped_state = np.append(reshaped_state, rsi_9)\n",
        "    reshaped_state = np.append(reshaped_state, sma_200d)\n",
        "    reshaped_state = np.append(reshaped_state, sma_100d)\n",
        "    reshaped_state = np.append(reshaped_state, sma_50d)\n",
        "    return reshaped_state\n",
        "\n",
        "class MyStocksEnv(StocksEnv):\n",
        "    def __init__(self, actual_ws, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.actual_ws = actual_ws\n",
        "        self.shape = (self.actual_ws + 5, )\n",
        "        INF = 1e10\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.last_action = 0\n",
        "\n",
        "        self.number_of_trades = 0\n",
        "        self.days_holding_position = 1\n",
        "        self.history_days_holding_position = []\n",
        "\n",
        "        self._total_reward = 0.\n",
        "        self.trade_fee_bid_percent = 0\n",
        "        self.trade_fee_ask_percent = 0\n",
        "\n",
        "\n",
        "    def is_trade(self, action):\n",
        "        if (action == Actions.Sell.value and self._position == Positions.Long):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def _calculate_reward(self, action):\n",
        "        step_reward = 0\n",
        "\n",
        "        if self.is_trade(action) or (self._truncated and self._position == Positions.Long):\n",
        "            self.number_of_trades += 1\n",
        "            current_price = self.prices[self._current_tick]\n",
        "            last_trade_price = self.prices[self._last_trade_tick]\n",
        "            step_reward = (current_price / last_trade_price) - 1\n",
        "\n",
        "        return step_reward\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed, options=options)\n",
        "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
        "\n",
        "        self.number_of_trades = 0\n",
        "        self.days_holding_position = 1\n",
        "        self.history_days_holding_position = []\n",
        "\n",
        "        self._truncated = False\n",
        "        self._current_tick = self._start_tick\n",
        "        self._last_trade_tick = self._current_tick - 1\n",
        "        self._position = Positions.Short\n",
        "        self._position_history = (self.window_size * [None]) + [self._position]\n",
        "        self._total_reward = 0.\n",
        "        self._total_profit = 1.  # unit\n",
        "        self._first_rendering = True\n",
        "        self.history = {}\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        observation = reshape_obs(observation, 0, self.actual_ws)\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == 'human':\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._truncated = False\n",
        "        self._current_tick += 1\n",
        "\n",
        "        if self._current_tick == self._end_tick:\n",
        "            self.history_days_holding_position.append(self.days_holding_position)\n",
        "            self._truncated = True\n",
        "\n",
        "        step_reward = self._calculate_reward(action)\n",
        "        self._total_reward += step_reward\n",
        "\n",
        "        self._update_profit(action)\n",
        "\n",
        "        trade = False\n",
        "        if (\n",
        "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
        "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
        "        ):\n",
        "            trade = True\n",
        "\n",
        "        if trade:\n",
        "            self._position = self._position.opposite()\n",
        "            self._last_trade_tick = self._current_tick\n",
        "\n",
        "        self._position_history.append(self._position)\n",
        "        observation = self._get_observation()\n",
        "        observation = reshape_obs(observation, action, self.actual_ws)\n",
        "        # print(observation)\n",
        "        info = self._get_info()\n",
        "        self._update_history(info)\n",
        "\n",
        "        if self.last_action == action:\n",
        "            self.days_holding_position += 1\n",
        "        else:\n",
        "            self.history_days_holding_position.append(self.days_holding_position)\n",
        "            self.days_holding_position = 1\n",
        "            self.last_action = action\n",
        "\n",
        "        if self.render_mode == 'human':\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, step_reward, self._truncated, self._truncated, info\n",
        "\n",
        "    def get_stats(self):\n",
        "        return self.number_of_trades, np.mean(self.history_days_holding_position), self.history_days_holding_position\n",
        "\n",
        "    def get_trend(self):\n",
        "        if self.prices[self.window_size] - self.prices[-1] > 0:\n",
        "            return \"decreasing\"\n",
        "        return \"rising\"\n",
        "\n",
        "    def render_all(self, max_days, title=None):\n",
        "        figsize_x = math.ceil((max_days / 100) * 3)\n",
        "        plt.figure(figsize=(figsize_x, 6))\n",
        "        window_ticks = np.arange(len(self._position_history))\n",
        "        plt.plot(self.prices)\n",
        "\n",
        "        short_ticks = []\n",
        "        long_ticks = []\n",
        "        last_pos = None\n",
        "        for i, tick in enumerate(window_ticks):\n",
        "            current_pos = self._position_history[i]\n",
        "\n",
        "            if current_pos == last_pos:\n",
        "                continue\n",
        "\n",
        "            if  current_pos == Positions.Short:\n",
        "                short_ticks.append(tick)\n",
        "            elif current_pos == Positions.Long:\n",
        "                long_ticks.append(tick)\n",
        "\n",
        "            last_pos = current_pos\n",
        "\n",
        "        plt.plot(short_ticks, self.prices[short_ticks], 'ro', label=\"Sprzedaj\")\n",
        "        plt.plot(long_ticks, self.prices[long_ticks], 'go', label=\"Kup\")\n",
        "\n",
        "        plt.legend()\n",
        "\n",
        "        if title:\n",
        "            plt.title(f\"Stocks from day {self.frame_bound[0]}\")\n",
        "\n",
        "        plt.suptitle(\n",
        "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
        "            \"Total Profit: %.6f\" % self._total_profit\n",
        "        )\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "HBZZ0vow4joS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 200\n",
        "start = 200\n",
        "period = len(df_shares_INTC) - 200\n",
        "actual_ws = 100\n",
        "\n",
        "agent = Agent(state_space_n=actual_ws + 5, action_space_n=2)\n",
        "\n",
        "env = make_env(actual_ws, df_shares_INTC, window_size, start, period)\n",
        "anytrading_stats = AnytradingStatsCollector()\n",
        "\n",
        "episodes = 35\n",
        "for ep in range(episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    state = env.reset()[0]\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "\n",
        "        next_state, reward, truncated, done, info = env.step(action)\n",
        "        score += reward\n",
        "\n",
        "        agent.store_transition(state, action, reward, next_state, int(done))\n",
        "        state = next_state\n",
        "\n",
        "        agent.learn()\n",
        "\n",
        "        agent.update_epsilon()\n",
        "\n",
        "    anytrading_stats.store_episode_results(info['total_profit'], score, agent.epsilon)\n",
        "    anytrading_stats.print_stats()\n",
        "\n",
        "agent.save_model(ep)\n",
        "env.render_all(period, f\"Starting day: {start}\")\n",
        "\n",
        "anytrading_stats.print_stats_plots()\n",
        "create_plot(\"Loss\", \"Step\", \"Loss per step\", agent.q_model_loss)"
      ],
      "metadata": {
        "id": "64YXaHs44kGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shares_storage = {\n",
        "    \"amd\": df_shares_AMD,\n",
        "    \"meta\": df_shares_META,\n",
        "    \"google\": df_shares_GOOGLE,\n",
        "    \"cisco\": df_shares_CISCO,\n",
        "}\n",
        "\n",
        "stats = {\n",
        "    \"profits\": [],\n",
        "    \"num_of_trades\": [],\n",
        "    \"avg_days_holding_positions\": [],\n",
        "    \"trends\": {\n",
        "        \"decreasing\": {\n",
        "            \"counter\": 0,\n",
        "            \"num_of_profits\": 0,\n",
        "            \"profit_values\": [],\n",
        "        },\n",
        "        \"rising\": {\n",
        "            \"counter\": 0,\n",
        "            \"num_of_profits\": 0,\n",
        "            \"profit_values\": [],\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "for key, t_shares in shares_storage.items():\n",
        "  print(f\"--------------------------{key}--------------------------\")\n",
        "  for i in range(200, len(t_shares) - 365, 100):\n",
        "      env = make_env(actual_ws, t_shares, window_size, i, 365)\n",
        "      obs = env.reset()[0]\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "          action = agent.choose_best_action(obs)\n",
        "          obs, reward, truncated, done, info = env.step(action)\n",
        "    #   env.render_all(365, True)\n",
        "\n",
        "      trend = env.get_trend()\n",
        "      num_of_trades, avg_days_holding_positions, _ = env.get_stats()\n",
        "      profit = info[\"total_profit\"]\n",
        "\n",
        "      stats['profits'].append(profit)\n",
        "      stats[\"num_of_trades\"].append(num_of_trades)\n",
        "      stats[\"avg_days_holding_positions\"].append(avg_days_holding_positions)\n",
        "      stats[\"trends\"][trend][\"counter\"] += 1\n",
        "      if profit >= 1.0:\n",
        "        stats[\"trends\"][trend][\"num_of_profits\"] += 1\n",
        "      stats[\"trends\"][trend][\"profit_values\"].append(profit)\n",
        "\n",
        "print(f\"--------------------------STATS--------------------------\")\n",
        "print(f\"Median profit: {np.median(stats['profits'])}\")\n",
        "print(f\"Lowest profit achived: {(np.min(stats['profits']) - 1) * 100}\")\n",
        "print(f\"Highest profit achived: {(np.max(stats['profits']) - 1) * 100}\")\n",
        "print(f\"Average num_of_trades: {np.mean(stats['num_of_trades'])}\")\n",
        "print(f\"Average avg_days_holding_positions: {np.mean(stats['avg_days_holding_positions'])}\")\n",
        "print(f\"Profits achvied on decreasing trend: {stats['trends']['decreasing']['num_of_profits']} / {stats['trends']['decreasing']['counter']}, {stats['trends']['decreasing']['num_of_profits'] / stats['trends']['decreasing']['counter'] * 100}%\")\n",
        "print(f\"Median profit achvied on decreasing trend: {np.median(stats['trends']['decreasing']['profit_values'])}, {(np.median(stats['trends']['decreasing']['profit_values']) - 1) * 100}%\")\n",
        "print(f\"Profits achvied on rising trend: {stats['trends']['rising']['num_of_profits']} / {stats['trends']['rising']['counter']}, {stats['trends']['rising']['num_of_profits'] / stats['trends']['rising']['counter'] * 100}%\")\n",
        "print(f\"Median profit achvied on rising trend: {np.median(stats['trends']['rising']['profit_values'])}, {(np.median(stats['trends']['rising']['profit_values']) - 1) * 100}%\")\n"
      ],
      "metadata": {
        "id": "TdXBdi8OVGgY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}